{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8SiTCuUAbIbhK7eJPwD99",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srika16/LLMs-from-scratch-working-copy/blob/main/ch02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwqxQ4364AIl",
        "outputId": "f2f9ea8a-0ffb-471d-ee4c-e9d84e72b7ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the-verdict.txt', <http.client.HTTPMessage at 0x7aeeeaac82d0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import urllib.request\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "print(\"Total number of characters: \", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5YhT8nX4qwM",
        "outputId": "a09b812b-34fd-4051-b023-411fc6b7c1f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters:  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list of individual words, whitespaces and punctionation characters\n",
        "import re\n",
        "text = \"Hello, World. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qOuiukD5PkH",
        "outputId": "85677388-7fd3-4db6-a421-6bf5b1f45e4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'World.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# regular expression splits on whitespaces(\\s), commas and periods ([,.])\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a45go6zP5mtB",
        "outputId": "b6f2857d-a7db-48d0-f89a-fe15467e61d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'World', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# whitespace free output\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPTlq38h7Y0J",
        "outputId": "92387f29-7722-43a1-e4f4-37d264c487ce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'World', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle other punctuation types\n",
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65gv5W9i7okE",
        "outputId": "18290549-90b0-472f-afb3-f8266f8f88d2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we have the proper regular expression, apply to the story\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))\n",
        "preprocessed[0:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm8KLgGr9W5B",
        "outputId": "d7ecd0fc-ccaa-42a2-b9d1-173c7a44ef09"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'HAD',\n",
              " 'always',\n",
              " 'thought',\n",
              " 'Jack',\n",
              " 'Gisburn',\n",
              " 'rather',\n",
              " 'a',\n",
              " 'cheap',\n",
              " 'genius',\n",
              " '--',\n",
              " 'though',\n",
              " 'a',\n",
              " 'good',\n",
              " 'fellow',\n",
              " 'enough',\n",
              " '--',\n",
              " 'so',\n",
              " 'it',\n",
              " 'was',\n",
              " 'no',\n",
              " 'great',\n",
              " 'surprise',\n",
              " 'to',\n",
              " 'me',\n",
              " 'to',\n",
              " 'hear',\n",
              " 'that',\n",
              " ',',\n",
              " 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert tokens to IDs\n",
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jeHpC4l-DoI",
        "outputId": "13604b01-1ad3-4c61-a164-b91551d52452"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a vocabulary\n",
        "vocab = {token:i for i,token in enumerate(all_words)}\n",
        "list(vocab.items())[:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuTPsQ_--fFd",
        "outputId": "cfe8b681-723c-4c9f-d3b3-f3678e3b4cf3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('!', 0),\n",
              " ('\"', 1),\n",
              " (\"'\", 2),\n",
              " ('(', 3),\n",
              " (')', 4),\n",
              " (',', 5),\n",
              " ('--', 6),\n",
              " ('.', 7),\n",
              " (':', 8),\n",
              " (';', 9),\n",
              " ('?', 10),\n",
              " ('A', 11),\n",
              " ('Ah', 12),\n",
              " ('Among', 13),\n",
              " ('And', 14),\n",
              " ('Are', 15),\n",
              " ('Arrt', 16),\n",
              " ('As', 17),\n",
              " ('At', 18),\n",
              " ('Be', 19),\n",
              " ('Begin', 20),\n",
              " ('Burlington', 21),\n",
              " ('But', 22),\n",
              " ('By', 23),\n",
              " ('Carlo', 24),\n",
              " ('Chicago', 25),\n",
              " ('Claude', 26),\n",
              " ('Come', 27),\n",
              " ('Croft', 28),\n",
              " ('Destroyed', 29)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[token] for token in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[id] for id in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "mdJj5Rp3AOqP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YkwA7DaBvJE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ4tBPHeBR2k",
        "outputId": "9bb8b07b-c8a3-4fd5-8dd0-826ad2415fa3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jikPvw_eBnIo",
        "outputId": "dc9f779a-6276-49e3-ffa5-a2e3125c62b1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Will not run, as \"Hello\" is not in the vocabulary\n",
        "# text = \"Hello, do you like Tea?\"\n",
        "# ids = tokenizer.encode(text)\n",
        "# print(ids)"
      ],
      "metadata": {
        "id": "xOzTtUuAC008"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add 2 special tokens <unk> unknown, <|endoftext|> end of the line text\n",
        "all_tokens = sorted(set(preprocessed))\n",
        "all_tokens = [\"<|endoftext|>\", \"<|unk|>\"] + all_tokens\n",
        "vocab = {token:i for i,token in enumerate(all_tokens)}\n",
        "print(list(vocab)[-5:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U2XjZQRDCdC",
        "outputId": "6c4c7891-cd43-4a5a-e672-068210767541"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['yet', 'you', 'younger', 'your', 'yourself']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    # replaces unknown words by <|unk|> tokens\n",
        "    preprocessed = [item if item in self.str_to_int\n",
        "                    else \"<|unk|>\" for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # Replace spaces before the specified punctuations\n",
        "    text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "16GIWOYh5Thq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like Tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join([text1, text2])\n",
        "print(text)\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INEJGbpG60MG",
        "outputId": "115c96ae-07fa-4d8e-9991-33087003e62c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like Tea? <|endoftext|> In the sunlit terraces of the palace.\n",
            "[1, 7, 357, 1128, 630, 1, 12, 0, 57, 990, 958, 986, 724, 990, 1, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7meAIVi7f2V",
        "outputId": "c9448921-888b-4b47-8653-d915f4e58ba2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like <|unk|>? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bype Pair Encoding is a more sophisticated tokenization schema based on a\n",
        "# concept called Bype Pair Encoding (BPE), used to train GPT2, GPT3.\n",
        "\n",
        "# We will use an implemention called tiktoken\n",
        "# Bype Pair Encoding can break down unknown words to smaller tokens so it will\n",
        "# never have unknown tokens.\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdvfld1S8BS5",
        "outputId": "c73fa53f-789e-4242-fad7-db41efa85e8b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(\"tiktoken version: \", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1u5OcIsH9jKU",
        "outputId": "32e135fb-31ed-4177-9367-6093971a19a3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version:  0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)\n",
        "print(tokenizer.decode(integers))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ-1_wv790E4",
        "outputId": "62a97b46-5ccb-4d04-9fda-8ebdf236e326"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 15777, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 20562, 13]\n",
            "Hello, do you like Tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    }
  ]
}